\mysection{Introduction}
The desirable optimization properties of submodular set functions have been
widely exploited in the design of approximate MAP estimation algorithms for
discrete conditional random fields (CRFs) \citep{boykov2001fast,
kumar2011improved}. Submodularity has also been recently used to design an
elegant variational inference algorithm to compute the marginals of a discrete
CRF by minimising an upper-bound on the log-partition function. In the initial
work of \citep{djolonga2014map}, the energy of the CRF was restricted to be
submodular. In a later work ~\citep{zhang2015higher}, the algorithm was
extended to handle more general Potts energy functions. The key idea there was to define a large ground set such that its subsets represent valid labelings, sublabelings or even incorrect labelings (these may assign two separate labels to a random variable and hence be invalid). Given the large ground set, it is possible to define a submodular set function whose value is equal to the energy of the CRF for subsets that specify a valid labeling of the model. We refer to such a set function as a {\em submodular extension} of the energy. 

For a given energy function, there exists a large number of possible submodular extensions. The accuracy of the variational inference algorithm depends crucially on the choice of the submodular extension. Yet, previous work has largely ignored the question of identifying accurate extensions for different energy classes. Indeed, the difficulty of identifying submodular extensions of general energy functions could be a major reason why the experiments of~\citep{zhang2015higher} were restricted to the special case of models specified by the Potts energy functions.

In this work, we establish a hitherto unknown connection between the submodular
extension of the Potts model proposed by Zhang et al.\citep{zhang2015higher}, and the
objective function of an accurate linear programming (LP) relaxation of the
corresponding MAP estimation problem~\citep{kleinberg2002approximation}. Specifically, the Lovasz extension of a submodular extension can be shown to be an objective function for the LP relaxation. This
connection has four important practical consequences. First, it establishes
the optimality of the submodular extension of the Potts model, via the tightest LP relaxation
under UGC-hardness assumptions. Second, it provides an
accurate submodular extension of the hierarchical Potts model, via the LP
relaxation of the corresponding MAP estimation problem proposed
by Kleinberg and Tardos \citep{kleinberg2002approximation}. Since any metric can be accurately
approximated as a mixture of hierarchical Potts
models~\citep{bartal1996probabilistic, bartal1998approximating}, this result
also provides a computationally feasible algorithm for estimating the marginals
for metric labeling. Third, it establishes the equivalence between the
subgradient of the LP relaxation and the conditional gradient of the problem of
minimising the upper bound of the log-partition. This allows us to employ the
widely used dense CRF, since the subgradient of its LP relaxation can be
efficiently computed using a recently proposed modified Gaussian filtering
algorithm~\citep{ajanthan2017efficient}. As a consequence, we provide the first
efficient algorithm to compute an upper bound of the log-partition function of
dense CRFs. This provides complementary information to the popular mean-field
inference algorithm for dense CRFs, which computes a lower bound on the
log-partition ~\citep{koltun2011efficient}. Fourth, we obtain an accurate 
submodular extension for a higher-order diversity model based on an LP
relaxation. Higher-order models can capture interesting properties of the image that cannot be
expressed using pairwise models~\citep{vineet2014filter, kohli2007p3}. We show that our upper-bounds on synthetic problems are comparable to those from tree reweighted message passing (TRW)
~\citep{wainwright2005new} for the case of sparse CRFs. Unlike our approach,
TRW is computationally infeasible for dense CRFs, thereby limiting its use in
practice. Using pairwise dense CRF models, we perform stereo matching on standard
data sets and obtain better results than \citep{koltun2011efficient}. We also perform semantic segmentation on the MSRC-21 dataset using a combination of dense pairwise and higher-order diversity model. The complete code is available at \url{https://github.com/pankajpansari/denseCRF}.

\mysection{Preliminaries}
\label{sec:prelim}

We now introduce the notation and definitions that we make use of in the remainder of the paper.

\myparagraph{Submodular Functions} Given a ground set $U = \left\{1, \dots, N \right\}$, denote by $2^U$ its power set. A set function $F: 2^U \to \mathbb{R}$ is {\it submodular} if, for all subsets $A, B \subseteq U$, we have
\begin{equation}
    F(A \cup B) + F(A \cap B) \leq F(A) + F(B).
\end{equation}
The set function $F$ is {\it modular} if there exists ${\bf s} \in \mathbb{R}^N$ such that $F(A) = \sum_{k \in A} s_k \ \forall \ A \subseteq 2^U$. Henceforth, we will use the shorthand $s(A)$ to denote ${\sum_{k \in A}} s_k$.

\myparagraph{Extended Polymatroid} Associated with any submodular function $F$ is a special polytope known as the {\it extended polymatroid} defined as
%
\begin{align}
EP(F) &= \{{\bf s} \in \mathbb{R}^N | \enskip \forall A \subseteq U: s(A) \leq F(A)\},
\label{eq:epf}
\end{align}
where $\bf s$ denotes the modular function $s(.)$ represented as a vector. 

\myparagraph{Lovasz Extension} For a given set function $F$ with $F(\emptyset) = 0$, the value of its Lovasz extension $f({\bf w}): \mathbb{R}^N \to \mathbb{R}$ is defined as follows: order the components of $\bf w$ in decreasing order such that $w_{j_1} \geq w_{j_2} \geq \dots \geq w_{j_N}$, where $(j_1, j_2, \dots, j_N)$ is the corresponding permutation of the indices. Then,
%
\begin{align}
    f({\bf w}) = \sum_{k = 1}^{N} w_{j_k} \left(F(j_1, \dots, j_k) - F(j_1,
    \dots, j_{k - 1})\right).
    \label{eq:lovasz_def}
\end{align}
%
The function $f$ is an extension because it equals $F$ on the vertices of the
unit cube. That is, for any $A \subseteq V$, $f({\bf 1}_A) = F(A)$, where ${\bf
1}_A$ is the $0\mhyphen{1}$ indicator vector corresponding to the elements of $A$.

{\prop By Edmond's greedy algorithm \citep{edmonds1970submodular}, if ${\bf w} \geq 0$ (non-negative elements), 
\begin{equation}
    f({\bf w}) = \max_{{\bf s} \in EP(F)} \langle {\bf w}, {\bf s} \rangle.
    \label{eq:lovasz_max}
\end{equation}
\label{prop:greedy}}

Property \ref{prop:greedy} implies that an LP over $EP(F)$ can be solved by computing the value of the Lovasz extension using equation (\ref{eq:lovasz_def}).

{\prop The Lovasz extension $f$ of a submodular function $F$ is a convex piecewise linear function.\label{prop:lovasz_convex}}

Property \ref{prop:lovasz_convex} holds since $f({\bf w})$ is the pointwise maximum of linear functions according to equation (\ref{eq:lovasz_max}).

\myparagraph{CRF and Energy Functions}
A CRF is defined as a graph on a set of random variables ${\mathcal X} = \{X_1,\dots, X_N\}$ related by a set of edges $\mathcal N$. We wish to assign every variable $X_a$ one of the labels from the set $\mathcal{L} = \{1, 2, \dots, L\}$. The quality of a labeling $\bf x$ is given by an energy function defined as 
%
\begin{equation}
    E({\bf x}) = \sum_{a \in {\mathcal X}} \phi_a(x_a) + \sum_{(a, b) \in {\mathcal N}} \phi_{ab}(x_a, x_b),
    \label{eq:energy_min}
  \end{equation}
%
where $\phi_a$ and $\phi_{ab}$ are the unary and pairwise potentials respectively. In computer vision, we often think of $\mathcal X$ as arranged on a grid. A \emph{sparse CRF} has $\mathcal N$ defined by 4-connected or 8-connected neighbourhood relationships. In a \emph{dense CRF}, on the other hand, every variable is connected to every other variable.

We can augment the above pairwise models with higher-order potentials. Let
$\mathcal C$ be the set of cliques on subgroups of variables. These cliques
can, for instance, be the set of superpixels from a clustering method, such as
$k$-means or mean-shift \cite{comaniciu2002mean}. Also, let ${\bf x}_c$ be the component of $\bf x$ formed by elements in clique $c$. The energy function now also contains higher-order potentials 
%
\begin{equation}
    E({\bf x}) = \sum_{a \in {\mathcal X}} \phi_a(x_a) + \sum_{(a, b) \in {\mathcal N}} \phi_{ab}(x_a, x_b) + \sum_{c \in {\mathcal C}} \phi_c({{\bf x}_c}),
    \label{eq:energy_hop_min}
  \end{equation}
%
 
The energy function can be interpreted as defining a probability distribution $P({\bf x})$ as follows:
\begin{equation}
    P({\bf x}) =
    \begin{cases}
        \frac{1}{\mathcal{Z}} \exp(-E({\bf x})) & \quad \text{if} \enskip {\bf x} \in {\mathcal L}^N,\\
       0  & \quad \text{otherwise}. \\
    \end{cases}
\label{eq:crf_prob_distrib}
\end{equation}
The normalization factor $Z = \sum_{{\bf x} \in {\mathcal L}^N} \exp(-E({\bf x}))$ is known as the {\it partition function}. 

\myparagraph{Inference} There are two inference problems in CRFs: 

(i) Marginal inference: We want to compute the marginal probabilities $P(X_a = i)$ for every $a = 1, 2, \dots, N$ and $i = 1, 2, \dots, L$.

(ii) MAP inference: We want to find a labeling with the minimum energy, that is, $\min_{{\bf x} \in {\mathcal L}^N} E({\bf x})$. Equivalently, MAP inference finds the mode of $P({\bf x})$.

\mysection{Review: Variational Inference Using Submodular Extensions}
\label{sec:review}
We now summarise the marginal inference method of Zhang et al. \citep{zhang2015higher} which
made use of a submodular extension.

\begin{figure}
\centering
\includegraphics[scale = 0.35]{./figures/1_of_L.png}
\mycaption{\footnotesize \em Illustration of $1$-of-$L$ encoding used in \citep{zhang2015higher} with 4 variables and 3 labels. The blue labeling, corresponding to $X_1 = 1, X_2 = 3, X_3 = 2, X_4 = 1$, is valid. The yellow labeling, corresponding to $X_2 = 2, X_3 = 1, 3, X_4 = 3$, is invalid since $X_3$ has been assigned multiple labels and $X_1$ has been assigned none.} 
\label{fig:encoding}
\end{figure}

\myparagraph{Submodular Extensions} A submodular extension is defined using
a ground set such that some of its subsets correspond to valid CRF labelings.
Note that not every subset needs to represent a valid labeling - some of them could
correspond to incomplete or invalid labelings. To obtain such an extension, we need an encoding scheme which gives the sets corresponding to valid CRF labelings. 

One example of an encoding scheme is the $1$-of-$L$ encoding, illustrated in figure \ref{fig:encoding}. Let each variable $X_a$ take one of $L$ possible labels. In this scheme, we represent the set of possible assignments for $X_a$ by the set $V_a = \{v_{a1}, v_{a2}, \dots, v_{aL}\}$. If $X_a$ is assigned label $i$, then we select the element $v_{ai}$. Extending to all variables, our ground set becomes $V = \cup_{a = 1}^N V_a$. A valid assignment $A \subseteq V$ assigns each variable exactly one label, that is, $|A \cap V_a| = 1$ for all $V_a$. We denote the set of valid assignments by $\mathcal M$ where $\mathcal{M} = \cap_{a = 1}^{N} \mathcal{M}_a$ and $\mathcal{M}_a = \{A: |A \cap V_a| = 1\}$.


Using our ground set $V$, we can define a submodular function $F$ which equals $E(\bf x)$ for all sets corresponding to valid labelings, that is, $F(A_{\bf x}) = E({\bf x}),\ {\bf x} \in {\mathcal L}^N$ where $A_{\bf x}$ is the set encoding of $\bf x$. We call such a function $F$ a {\it submodular extension} of $E({\bf x})$. 

\myparagraph{Upper-Bound on Log-Partition} Using a submodular extension $F$ and given any ${\bf s} \in EP(F)$, we can obtain an upper-bound on the partition function as
\begin{equation}
    {\mathcal Z} = \sum_{A \in {\mathcal M}} \exp(-F(A)) \leq \sum_{A \in {\mathcal M}} \exp(-s(A)),
\end{equation}
where $\mathcal M$ is the set of valid labelings. The upper-bound is the partition function of the distribution $Q(A) \propto \exp(-s(A))$, which factorises fully because $s(.)$ is modular. Since ${\bf s} \in EP(F)$ is a free parameter, we can obtain good approximate marginals of the distribution $P(\cdot)$ by minimising the upper-bound. After taking logs, we can equivalently write our optimisation problem as
\begin{equation}
  \min_{{\bf s} \in EP(F)} g({\bf s}), \text{where } g({\bf s}) = \log \sum_{A \in {\mathcal M}} \exp(-s(A)).
\label{eq:min_upper_bound}
\end{equation}

\myparagraph{Conditional Gradient Algorithm} The conditional gradient algorithm
(algorithm \ref{algo:fw_inference}) \citep{frank1956algorithm} is a good
candidate for solving problem (\ref{eq:min_upper_bound}) due to two reasons.
First, problem \eqref{eq:min_upper_bound} is convex. Second, as solving an LP
over EP(F) is computationally tractable (property \ref{prop:greedy}), the
conditional gradient can be found efficiently. The algorithm starts with an
initial solution $\bf s_0$ (line \ref{line:initial}). At each iteration, we
compute the conditional gradient $\bf s^*$ (line \ref{line:cond_grad}), which
minimises the linear approximation $g({\bf s}_k) + \nabla g({\bf s}_k)^T ({\bf
s} - {\bf s}_k)$ of the objective function. Finally, $\bf s$ is updated by
either (i) fixed step size schedule, as in line \ref{line:step_size} of
algorithm \ref{algo:fw_inference}, or (ii) by doing a line search and setting ${\bf s}_{k+1} = \min_{0 \leq \gamma \leq 1} g(\gamma {\bf s}^* + (1 - \gamma) {\bf s}_{k})$.

\begin{algorithm}
\mycaption{Upper Bound Minimisation using Conditional Gradient Descent}
\label{algo:fw_inference}
\begin{algorithmic}[1]
    \STATE Initialize ${\bf s} = {\bf s}_0 \in EP(F)$ \label{line:initial}
\FOR{k = 1 to \texttt{MAX\_ITER}}
\STATE ${\bf s^*} = \argmin_{{\bf s} \in EP(F)} \langle \nabla g({\bf s}_k), {\bf s} \rangle$ \label{line:cond_grad}
\IF{$\langle {\bf s^* - s}_k, \nabla g({\bf s}_k) \rangle \leq \epsilon$}
\STATE {\bf break}
\ENDIF
\STATE ${\bf s}_{k + 1} = {\bf s}_k + \gamma({\bf s^*} - {\bf s}_k)$ \text{with} $\gamma = 2/(k + 2)$ \label{line:step_size}
\ENDFOR   
\RETURN $\bf s$
\end{algorithmic}
\end{algorithm}

\mysection{Overview: Accurate Submodular Extensions from LP Relaxations}

Different choices of extensions $F$ change the domain in problem \eqref{eq:min_upper_bound}, leading to different upper bounds on the log-partition function. How does one come up with accurate extensions for different classes of CRF energy functions? Is it possible to identify optimal extensions which yield the tightest bound? 

If we introduce a temperature parameter in $P({\bf x})$ (equation
\eqref{eq:crf_prob_distrib}) by using $E({\bf x})/T$ and decrease $T$, the
resulting distribution starts to peak more sharply around its mode. This
is assuming that $P({\bf x})$ is unimodal, and there aren't multiple solutions
with the same minima. As $T \to 0$, marginal estimation becomes the same as MAP inference since the resulting
distribution $P^0({\bf x})$ has mass 1 at its mode ${\bf x}^*$ and is
0 everywhere else. Given the MAP solution ${\bf x}^*$, one can compute the
marginals as $P(X_i = j) = [x_i^* = j]$, where [.] is the Iverson bracket. Motivated by this connection, we ask if one can introduce a temperature parameter to our problem \eqref{eq:min_upper_bound} and transform it to an LP relaxation in the limit $T \to 0$? We can then hope to use accurate LP relaxations of MAP problems known in literature to find good submodular extensions for different classes of energy functions. 

We answer this question in affirmative. Specifically, in the following two sections we show how one can select the set encoding and submodular extension to convert problem \eqref{eq:min_upper_bound} to accurate LP relaxations for Potts, hierarchical Potts and higher-order diversity models. In fact, when the LP relaxation has tightness guarantees, we obtain worst-case optimal submodular extensions - a notion we elucidate shortly. 

In this work, we focus on submodular extension families $\mathcal F(.)$
corresponding to different classes of energy functions $\mathcal E$. Given an
instance of the energy function $E(.)$ from the class $\mathcal E$,  there is
a corresponding submodular extension ${\mathcal F}(E)$. Moreover, we seek a closed-form analytical expression for $\mathcal F$. 

\mysection{Worst-case Optimal Submodular Extensions}

\myparagraph{Worst-case Optimality}

Our extensions are derived from LP relaxations. When the LP relaxation is the
tightest possible, the extension family ${\mathcal
F}_{opt}$ we obtain is \emph{worst-case optimal}. This is to say that there does not
exist another submodular extension family ${\mathcal F}$ that gives a tighter
upper bound for problem \eqref{eq:min_upper_bound} than ${\mathcal F}_{opt}$
for all instances of the energy function in $\mathcal E$. Formally, 
\begin{equation}
    \nexists {\mathcal F}: \min_{{\bf s} \in EP({\mathcal F}(E))} g({\bf s})
    \leq \min_{{\bf s} \in EP({\mathcal F}_{opt}(E))} g({\bf s}) \enspace
    \forall \; E(.) \in \mathcal E.
    \label{eq:best_submod}
\end{equation}
Note that the notion of worst-case optimality does not guarantee that the
extension is best for every instance of the energy function. It may be possible
to solve an optimisation problem to obtain the best extension for every given
instance of energy $E(.)$. However, in this paper, we do not take that approach, and instead provide a general solution for a given class of energy function. 

In subsection \ref{subsec:potts}, we prove the worst-case optimality of the submodular extension used in literature for the Potts model. In subsection \ref{subsec:hier_potts}, we obtain worst-case optimal submodular extension for the more general hierarchical Potts model. Finally, in subsection \ref{subsec:dense_crf}, we provide an approach to make our inference algorithm efficient for the dense CRF model. Using our approach, the inference algorithm has linear time-complexity in the number of variables and labels.

\mysubsection{Pairwise Potts Model}
\label{subsec:potts}
The pairwise Potts model, also known as the uniform metric, specifies the pairwise potentials $\phi_{ab}(x_a, x_b)$ in equation (\ref{eq:energy_min}) as follows:
\begin{equation}
    \phi_{ab}(x_a, x_b) = w_{ab} \cdot [x_a \neq x_b],
\end{equation}
where $w_{ab}$ is the weight associated with edge $(a, b)$. There are no higher-order potential terms in this model as in equation (\ref{eq:energy_min}).
%
\myparagraph{Tightest LP Relaxation} Before describing our set encoding and
submodular extension, we briefly outline the LP relaxation of the corresponding
MAP estimation problem. To this end, we define indicator variables $y_{ai}$
which equal 1 if $X_a = i$, and 0 otherwise. The following LP relaxation is the
tightest possible for Potts model in the worst-case, assuming the Unique Games
Conjecture is true \citep{manokaran2008sdp}:
\begin{align}
    \text{(P-LP)} \quad \min_{\bf y} \enskip & E({\bf y}) =  \sum_{a} \sum_{i} \phi_a(i)y_{ai} + \nonumber\\
     &\sum_{(a, b) \in {\mathcal N}} \sum_{i} \frac{w_{ab}}{2} \cdot |y_{ai} - y_{bi}| \nonumber\\
    \text{s.t} \quad &{\bf y} \in \Delta.
\label{eq:p-lp}
\end{align}
The set $\Delta$ is the union of $N$ probability simplices:
\begin{equation}
    \Delta = \{{\bf y}_a \in \mathbb{R}^L | {\bf y}_a \geq 0  \textrm{ and } \langle {\bf 1}, {\bf y}_a \rangle = 1\},
    \label{eq:delta}
\end{equation}
where $\bf y$ is the vector of all variables and ${\bf y}_a$ is the component
of $\bf y$ corresponding to $X_a$.


\myparagraph{Set Encoding} We choose to use the $1$-of-$L$ encoding for Potts model as described in section \ref{sec:review}. With the encoding scheme for Potts model above, $g({\bf s})$ can be factorised and problem (\ref{eq:min_upper_bound}) can be rewritten as:
%
\begin{equation}
\min_{{\bf s} \in EP(F)} \quad \sum_{a = 1}^{N} \log \sum_{i = 1}^L \exp(-s_{ai}).
\label{eq:potts_upper_bound}
\end{equation}
(See Remark 1 in appendix A for proof)
\myparagraph{Marginal Estimation with Temperature} We now introduce
a temperature parameter $T > 0$ to problem \eqref{eq:potts_upper_bound} which
divides $E({\bf x})$, or equivalently divides $\bf s$ belonging to $EP(F)$. Also, since $T > 0$, we can multiply the objective by $T$ leaving the problem unchanged. Without changing the solution, we can transform problem \eqref{eq:potts_upper_bound} as follows
\begin{equation}
    \min_{{\bf s} \in EP(F)} \quad g_T ({\bf s}) = \sum_{a = 1}^{N} T \cdot \log \sum_{i = 1}^L \exp(-\frac{s_{ai}}{T}).
\label{eq:potts_temp}
\end{equation}

\begin{figure}
\centering
\includegraphics[scale = 0.35]{./figures/potts_chain_extension.png}
\mycaption{\footnotesize \em An illustration of the worst-case optimal
submodular extension for Potts model for a chain graph of 4 variables, each of
which can take 3 labels. The figure shows the way to compute the extension
values of the set $A = \{v_{1, 1}, v_{4, 1}, v_{3, 2}$\}.}
\label{fig:potts_extension_example}
\end{figure}

\begin{figure}
\centering
\resizebox{180pt}{150pt}{
\begin{tikzpicture}[-latex ,auto, node distance =1.5 cm and 1.5cm ,on grid
  , semithick , state/.style ={ circle , color = black , draw,black , text=blue , minimum width =1 cm}, cross/.style={color=red, very thick, dashed}, unary/.style={color=blue, very thick}] 

  \node[state] (n11) at (1, 1) {\Large $1\mhyphen1$};
  \node[state] (n12) at (3, 1) {\Large $1\mhyphen2$};
  \node[state] (n13) at (5, 1) {\Large $1\mhyphen3$};
  \node[state] (n21) at (7, 1) {\Large $2\mhyphen1$};
  \node[state] (n22) at (9, 1) {\Large $2\mhyphen2$};
  \node[state] (n23) at (11, 1) {\Large $2\mhyphen3$};
  \node[state] (source) at (6, 3) {\Large s};
  \node[state] (sink) at (6, -3) {\Large t};

% \begin{scope}[ every node/.style={fill=white,circle}]
\path[unary] (n11) edge [bend right =25] node [pos=0.20, anchor=west] {\Large$\phi_1(1)$} (sink);
\path[unary] (n12) edge [bend right =25] node [pos=0.20, anchor=west]{\Large$\phi_1(2)$} (sink);
\path[unary] (n13) edge [bend right =25] node [pos=0.20, anchor=west]{\Large$\phi_1(3)$} (sink);
\path[unary] (n21) edge [bend left =25] node [pos=0.20, anchor=west]{\Large$\phi_2(1)$} (sink);
\path[unary] (n22) edge [bend left =25] node [pos=0.20, anchor=west]{\Large$\phi_2(2)$} (sink);
\path[unary] (n23) edge [bend left =25] node [pos=0.20, anchor=west]{\Large$\phi_2(3)$} (sink);
%\end{scope}
 
\path[cross] (n11) edge [bend left =25] node[above] {} (n21);
\path[cross] (n21) edge [bend left =25] node[below] {} (n11);

\path[cross] (n12) edge [bend left =25] node[above] {} (n22);
\path[cross] (n22) edge [bend left =25] node[below] {} (n12);

\path[cross] (n13) edge [bend left =25] node[above] {} (n23);
\path[cross] (n23) edge [bend left =25] node[below] {} (n13);
\end{tikzpicture}
}
\mycaption{\footnotesize \em An st-graph specifying the worst-case optimal submodular
extension for Potts model for 2 variables with 3 labels each and connected to
each other. There is a node for each variable and each label, that is, for all
elements of the ground set. The nodes have been labeled as `variable-label',
hence node 1-1 represents the element $v_{11}$ and so on. The solid blue arcs
model the unary potentials, and the dotted red arcs represent the pairwise
potentials. Each dotted red arc has weight $w_{12}/2$.}
\label{fig:optimal_extension_graph}
\end{figure}


\myparagraph{Worst-case Optimal Submodular Extension} 
We now connect our marginal estimation problem (\ref{eq:min_upper_bound}) with
LP relaxations using the following proposition.
{\proposition Using the $1$-of-$L$ encoding scheme, in the limit $T \to 0$,
  problem (\ref{eq:potts_temp}) for Potts model becomes:
\begin{align}
    - \min_{{\bf y} \in \Delta} f({\bf y}) 
\end{align}
where $f(.)$ is the Lovasz extension of $F(.)$.
\label{proposition:potts_equiv}}

\begin{proof}
In the limit of $T \to 0$, we can rewrite the above problem as
\begin{equation}
    \min_{{\bf s} \in EP(F)} \quad \sum_{a = 1}^{N} \max_{i} (-s_{ai}).
\end{equation}
In vector form, the problem becomes
\begin{align}
    &\min_{{\bf s} \in EP(F)} \max_{{\bf y} \in \Delta} - \langle {\bf y}, {\bf s} \rangle \\
    & = - \max_{{\bf s} \in EP(F)} \min_{{\bf y} \in \Delta} \langle {\bf y}, {\bf s} \rangle
    \label{eq:maxmin}
\end{align}
where $\Delta$ is the domain as defined as in equation (\ref{eq:delta}). By the minimax theorem for LP, we can reorder the terms:
\begin{equation}
    - \min_{{\bf y} \in \Delta} \max_{{\bf s} \in EP(F)} \langle {\bf y}, {\bf s} \rangle 
    \label{eq:minmax}
\end{equation}
Recall that $\max_{{\bf s} \in EP(F)} \langle {\bf y}, {\bf s} \rangle$ is the value of the Lovasz extension of $F$ at $\bf y$, that is, $f({\bf y})$. Hence, as $T \to 0$, the marginal inference problem converts to minimising the Lovasz extension under the simplices constraint:
\begin{equation}
    - \min_{{\bf y} \in \Delta} f({\bf y}) 
    \label{eq:minLovasz}
\end{equation}
\end{proof}

The above problem is equivalent to an LP relaxation of the corresponding MAP
estimation problem (see Remark 2 in appendix). We note that $g_T ({\bf s})$ in
problem \eqref{eq:potts_temp} becomes the objective function of an LP
relaxation in the limit $T \to 0$. We seek to obtain the worst-case optimal
submodular extension by making $g_T ({\bf s})$ same as the objective of (P-LP)
as $T \to 0$. We note that problems \eqref{eq:potts_temp} and
\eqref{eq:potts_upper_bound} become equivalent at $T = 1$, and our worst-case optimality
guarantee was derived in the limit $T \to 0$. However, it is possible to
derive such a guarantee only in the limit case, and it is in this sense that we
claim the worst-case optimality.

The question now becomes how to recover the worst-case optimal submodular extension using $E({\bf y})$. The following proposition answers this question.
{\proposition The worst-case optimal  submodular extension for Potts model is given by $F_{Potts}(A) = \sum_{i = 1}^L F_i(A)$, where
\begin{align}
    F_i(A) &= \sum_a \phi_{a}(i) [|A \cap \{v_{ai}\}| = 1] + \nonumber \\
           &\sum_{(a, b) \in {\mathcal N}} \frac{w_{ab}}{2} \cdot [|A \cap \{v_{ai}, v_{bi}\}| = 1]
\end{align}
Also, $E({\bf y})$ in (P-LP) is the Lovasz extension of $F_{Potts}$.
\label{proposition:potts_worst-case optimal}}
\begin{proof}
Since $F_{Potts}$ is sum of Ising models $F_i$, we first focus on a particular label $i$ and then generalize. Consider a graph with only two variables $X_a$ and $X_b$ with an edge between them. The ground set in this case is $\{v_{ai}, v_{bi}\}$. Let the corresponding relaxed indicator variables be ${\bf y} = \{y_{aj}, y_{bj}\}$, such that $y_{ai}, y_{bi} \in [0, 1]$ and assume $y_{ai} > y_{bi}$. The Lovasz extension in this case is:

\begin{align}
f({\bf y}) &= y_{ai} \cdot [F_i(\{v_{ai}\}) - F_i(\{\})] + y_{bi} \cdot [F_i(\{v_{ai}, v_{bi}\}) - F_i(\{v_{ai}\})] \nonumber \\
&= y_{ai} \cdot [\left(\phi_{a}\left(j\right) + \frac{w_{ab}}{2}\right) - 0] + y_{bi} \cdot [\left(\phi_{a}\left(j\right) + \phi_{b}\left(j\right)\right) - \left(\phi_{a}\left(j\right) + \frac{w_{ab}}{2}\right)] \nonumber \\
&=  \phi_{a}\left(j\right) \cdot y_{ai} + \phi_{b}\left(j\right) \cdot y_{bi} + \frac{w_{ab}}{2} \cdot \left(y_{ai} - y_{bi}\right)
\end{align}

In general for both orderings of $y_{ab}$ and $y_{bi}$, we can write
%
\begin{equation}
f({\bf y}) = \phi_{a}(j) \cdot y_{ai} + \phi_{b}(j) \cdot y_{bi} + \frac{w_{ab}}{2} \cdot |y_{ai} - y_{bi}|
\label{eq:a-lovasz_ising}
\end{equation}
%
Extending the Lovasz extension of equation \eqref{eq:a-lovasz_ising} to all variables and all labels gives us $E({\bf y})$ in (P-LP). 

\end{proof}
\vspace{-1mm}
Proposition \ref{proposition:potts_worst-case optimal} paves the way for us to identify the worst-case optimal extension for hierarchical Potts model, which we discuss in the following section. Figure (\ref{fig:potts_extension_example}) shows an example where we compute $F_{Potts}(A)$ for a small graph and a given set $A$ representing an invalid labeling. Figure (\ref{fig:optimal_extension_graph}) shows the $st$-graph representation of $F_{Potts}$ for a small instance.

\mysubsection{Hierarchical Potts Model}
\label{subsec:hier_potts}

Potts model imposes the same penalty for unequal assignment of labels to
neighbouring variables, regardless of the label dissimilarity. In some
scenarios, a more natural approach is to vary the penalty based on how different the labels are. A hierarchical Potts model permits this by specifying the distance between labels using a tree with the following properties:
\vspace{2mm}
\begin{enumerate}
  \itemsep+1em 
\item The vertices are of two types: (i) the leaf nodes representing labels, and (ii) the non-leaf nodes, except the root, representing meta-labels.
\item The lengths of all the edges from a parent to its children are the same.
\item The lengths of the edges along any path from the root to a leaf decreases by a factor of at least $r \geq 2$ at each step.
\item The metric distance between nodes of the tree is the sum of the edge lengths on the unique path between them. 
\end{enumerate}
\vspace{2mm}
%
\begin{figure}
\centering
\includegraphics[scale = 0.35]{./figures/rhst_notation.png}
\mycaption{\footnotesize \em A hierarchical Potts model instance illustrating notations with 2 meta-labels (blue) and 4 labels (yellow). All labels are at the same level. $r = 2$, that is, edge-length decreases by 2 at each level. Also, distance between labels 1 and 3 is $d(1, 3) = 0.5 + 1 + 1 + 0.5 = 3$.} 
\label{fig:rhst}
\end{figure}

A subtree $T$ of an hierarchical Potts model is a tree comprising all the descendants of some node $v$ (not root). Given a subtree $T$, $l_T$ denotes the length of the tree-edge leading upward from the root of $T$ and $L(T)$ denotes the set of leaves of $T$. We call the leaves of the tree as labels and all other nodes of the tree expect the root as \emph{meta-labels}. Figure \ref{fig:rhst} illustrates the notations in the context of a hierarchical Potts model.
%
\myparagraph{Tightest LP Relaxation} We use the same indicator variables
$y_{ai}$ that were employed in the LP relaxation of Potts model. Let $y_a(T)
= \sum_{i \in L(T)} y_{ai}$. The following LP relaxation is the tightest known
for hierarchical Potts model in the worst-case, assuming the Unique Games
Conjecture is true \citep{manokaran2008sdp}
\begin{align}
\text{(T-LP)} \quad \min_{\bf y} \enskip & \widetilde{E}({\bf y}) =  \sum_{a} \sum_{i} \phi_a(i)y_{ai} + \nonumber\\
                                         &\sum_{(a, b) \in {\mathcal N}} w_{ab} \sum_T l_T \cdot |y_{a}(T) - y_{b}(T)| \nonumber\\
    \text{such that} \quad &{\bf y} \in \Delta.
\label{eq:t-lp}
\end{align}
The set $\Delta$ is the same domain as defined in equation (\ref{eq:delta}). 

\myparagraph{Transformed Tightest LP Relaxation} We take (T-LP) and rewrite it using indicator variables for all labels and meta-labels. Let $\mathcal R$ denote the set of all labels and meta-labels, that is, all nodes in the tree apart from the root. Also, let $\mathcal L$ denote the set of labels, that is, the leaves of the tree. Let $T_i$ denote the subtree which is rooted at the $i$-th node. We introduce an indicator variable $z_{ai} \in \{0, 1\}$, where
\begin{align}
    \enskip  z_{ai} =  \begin{cases} 
        y_{ai} \quad &\text{if} \enskip i \in {\mathcal L} \\
        y_{a}(T_i) \quad &\text{if} \enskip i \in {\mathcal R - \mathcal L} \\
    \end{cases}
\end{align}

We need to extend the definition of unary potentials to the expanded label space as follows:
\begin{align}
    \text{where} \enskip    \phi'_{a}(i) =  \begin{cases} 
        \phi_{a}(i) \quad &\text{if} \enskip i \in {\mathcal L} \\
        0  \quad &\text{if} \enskip i \in {\mathcal R - L} \\
    \end{cases}
\end{align}
%
We can now rewrite problem \eqref{eq:t-lp} in terms of new indicator variables $z_{ai}$:
%
\begin{align}
    \text{(T-LP-FULL)} \quad &\min \widetilde{E}({\bf z}) = \sum_{i \in \mathcal{R}} \sum_{a \in {\mathcal X}} \phi_a'(i) \cdot z_{ai} + \nonumber \\
            &\sum_{i \in \mathcal{R}} \sum_{(a, b) \in {\mathcal N}} w_{ab} \cdot l_{T_i} \cdot |z_{ai} - z_{bi}| \nonumber \\
    \text{such that} \quad {\bf z} \in \Delta'
\label{eq:t-lp-full}
\end{align}
   where $\Delta'$ is the convex hull of the vectors satisfying
\begin{align}
    \enskip &\sum_{i \in \mathcal{L}} z_{ai} = 1, \enskip z_{ai} \in \{0, 1\} \enskip \forall a \in {\mathcal X}, i \in \mathcal{L} \\
    \text{and} \enskip &z_{ai} = \sum_{j \in L(T_i) } z_{aj}.\enskip \forall a \in {\mathcal X}, i \in \mathcal{R - L} \label{eq:consistency_constraint} 
\end{align}
%
Constraint (\ref{eq:consistency_constraint}) ensures consistency among labels and meta-labels, that is, if a label is assigned then all the meta-labels which lie on the path from the root to the label should be assigned as well. 
%
\myparagraph{Set Encoding} For any variable $X_a$, let the set of possible assignment of labels and meta-labels be the set $V_a = \{v_{a1}, \dots, v_{aM}\}$, where $M$ is the total number of nodes in the tree except the root. Our ground set is $V = \cup_{a = 1}^{N} V_a$ of size $N \cdot M$.

A consistent labeling of a variable assigns it one label, and all meta-labels on the path from root to the label. Let us represent the set of consistent assignments for $X_a$ by the set $P_a = \{p_{a1}, \dots, p_{aL}\}$, where $p_{ai}$ is the collection of elements from $V_a$ for label $i$ and all meta-labels on the path from root to label $i$. The set of valid labelings $A \subseteq V$ assigns each variable exactly one consistent label. This constraint can be formally written as $\mathcal{M} = \cap_{a = 1}^{N} \mathcal{M}_a$ where $\mathcal{M}_a$ has exactly one element from $P_a$. Let $s'_{ai}$ be the sum of the components of $\bf s$ corresponding to the elements
of $p_{ai}$, that is, 
\begin{equation}
s'_{ai} = \sum_{t \in p_{ai}} s_t.
\label{eq:tree_s}
\end{equation}
Using our encoding scheme, we rewrite problem (\ref{eq:min_upper_bound}) as:
\begin{equation}
\min_{{\bf s} \in EP(F)} \quad \sum_{a = 1}^{N} \log \sum_{i = 1}^L \exp(-s'_{ai}).
\label{eq:metric_upper_bound}
\end{equation}
%
\myparagraph{Marginal Estimation with Temperature} Similar to Potts model, we now introduce a temperature parameter $T > 0$ to problem \eqref{eq:metric_upper_bound}. The transformed problem becomes
\begin{equation}
    \min_{{\bf s} \in EP(F)} \quad g_T ({\bf s}) = \sum_{a = 1}^{N} T \cdot \log \sum_{i = 1}^L \exp(-\frac{s'_{ai}}{T}).
\label{eq:metric_temp}
\end{equation}
\myparagraph{Worst-case Optimal Submodular Extension} The following proposition connects the marginal estimation problem (\ref{eq:min_upper_bound}) with LP relaxations:

{\proposition In the limit $T \to 0$, problem (\ref{eq:metric_temp}) for hierarchical Potts energies becomes:
\begin{align}
    - \min_{{\bf z} \in \Delta'} f({\bf z}) 
\end{align}
\label{proposition:metric_equiv}}
(Proof in appendix).

The above problem is equivalent to an LP relaxation of the corresponding MAP
estimation problem (see Remark 3 in appendix). Hence, $g_T ({\bf s})$ becomes the objective function of an LP relaxation in the limit $T \to 0$. We seek to make this objective same as $\widetilde{E}({\bf z})$ of (T-LP-FULL) in the limit $T \to 0$. The question now becomes how to recover the  worst-case optimal submodular extension from $\widetilde{E}({\bf z})$.
%
{\proposition The  worst-case optimal submodular extension for hierarchical Potts model is given by $F_{hier}(A) = \sum_{i = 1}^M F_i(A)$, where
\begin{align}
    F_i(A) &= \sum_a \phi'_{a}(i) [|A \cap \{v_{ai}\}| = 1] + \nonumber \\
           &\sum_{(a, b) \in {\mathcal N}} {w_{ab}} \cdot l_{T_i} \cdot [|A \cap \{v_{ai}, v_{bi}\}| = 1]
\end{align}
Also, $\widetilde{E}({\bf z})$ in (T-LP-FULL) is the Lovasz extension of $F_{hier}$.
\label{proposition:rhst_worst-case optimal}}
(Proof in appendix)

Since any finite metric space can be probabilistically approximated by mixture
of tree metric \citep{bartal1996probabilistic}, the worst-case optimal
submodular extension for metric energies can be obtained using $F_{hier}$. Note
that $F_{hier}$ reduces to $F_{Potts}$ for Potts model. One can see this by
considering the Potts model as a star-shaped tree with edge weights as 0.5.

\mysubsection{Fast Conditional Gradient Computation for Dense CRFs}
\label{subsec:dense_crf}

\myparagraph{Dense CRF Energy Function} A dense CRF is specified by the following energy function 
\begin{equation}
    E({\bf x}) = \sum_{a \in {\mathcal X}} \phi_a(x_a) + \sum_{a \in {\mathcal X}} \sum_{b \in {\mathcal X}, b \neq a} \phi_{ab}(x_a, x_b).
\end{equation}
Note that every random variable is a neighbour of every other random variable in a dense CRF. Similar to previous work \citep{koltun2011efficient}, we consider the pairwise potentials to be to be Gaussian, that is, 
\begin{align}
    &\phi(i, j) = \mu(i, j) \sum_{m} w^{(m)} k({\bf f}_a^{(m)}, {\bf f}_b^{(m)}), \\
    &k({\bf f}_a^{(m)}, {\bf f}_b^{(m)}) = \exp\left( \frac{- ||{\bf f}_a - {\bf f}_b ||^2}{2} \right).
\end{align}
The term $\mu(i,j)$ is known as {\it label compatibility} function between
labels $i$ and $j$. Potts model and hierarchical Potts models are examples of
$\mu(i, j)$. The other term is a mixture of Gaussian kernels $k(.,.)$ and is
called the {\it pixel compatibility} function. The terms ${\bf f}_a^{(m)}$ are
features that describe the random variable $X_a$. In practice, similar to
\citep{koltun2011efficient}, we use $x, y$ coordinates and RGB values
associated to a pixel as its features.

Algorithm \ref{algo:fw_inference} assumes that the conditional gradient $\bf s^*$ in step {\ref{line:cond_grad}} can be computed efficiently. This is certainly not the case for dense CRFs, since computing $s^*$ involves $NL$ function evaluations of the submodular extension $F$, where $N$ is the number of variables, and $L$ is the number of labels. Each $F$ evaluation has complexity ${\mathcal O}(N)$ using the efficient Gaussian filtering algorithm of \citep{koltun2011efficient}. However, computation of $s^*$ would still be ${\mathcal O} (N^2)$, which is clearly impractical for computer-vision applications where $N \sim 10^5 - 10^6$.

However, using the equivalence of relaxed LP objectives and the Lovasz
extension of submodular extensions in proposition
\ref{proposition:potts_equiv}, we are able to compute $s^*$ in ${\mathcal
O}(NL)$ time. Specifically, we use the algorithm of
Ajanthan et al. \citep{ajanthan2017efficient}, which provides an efficient filtering procedure to compute the subgradient of the LP relaxation objective $E({\bf y})$ of (P-LP).\\
%
\vspace{-0.5cm}
{\proposition Computing the subgradient of $E({\bf y})$ in (P-LP) is equivalent to computing the conditional gradient for the submodular function $F_{Potts}$. \label{proposition:subgrad}} 
\begin{proof}
  For the Potts model, we derived the worst-case optimal extension $F_{Potts}$ by
  making its Lovasz extension $f({\bf y})$ same as the objective function
    $E({\bf y})$ of the worst-case optimal LP relaxation. Hence, we have
\begin{align}
    E({\bf y}) &= f({\bf y})  \nonumber \\
    {} &=  \max_{{\bf s} \in EP(F)} \langle {\bf y}, {\bf s} \rangle.\nonumber
\end{align}

The subdifferential of $E({\bf y})$ at $\bf y_0$ is the `active' linear function at $\bf y_0$. Hence,
\begin{align}
    \partial E({\bf y})|_{\bf y = y_0} &=  \argmax_{{\bf s} \in EP(F)} \langle {\bf y_0}, {\bf s} \rangle
    \label{eq:subgrad}
\end{align}
Equation (\ref{eq:subgrad}) is equivalent to ${\bf s}^*$ computation in line \ref{line:cond_grad} of algorithm \ref{algo:fw_inference}, with ${\bf y_0} = - \nabla g({\bf s}_k)$.
\end{proof}

A similar observation can be made in case of hierarchical Potts model. Hence we have the first practical algorithm to compute upper bound of log-partition function of a dense CRF for Potts and metric energies.

\mysection{Accurate Submodular Extension for Higher-order Diversity Model} The pairwise Potts model often fails
to capture useful image statistics because it restricts the order of the
potentials to be at most two. Higher order clique potentials can model complex
interactions of random variables, and thereby overcome this difficulty.

A higher-order model useful in real-world applications is the
\textit{diversity} model, which favours labeling where variables in a clique
have fewer number of unique labels. For instance, in semantic segmentation we
often first obtain superpixels from a clustering method \cite{comaniciu2002mean}, which we consider as cliques in our model. We expect the labeling in a superpixel to be homogeneous. As a result, it is preferable to have pixels belonging to a superpixel to be incorrectly labeled with two class labels rather than three or more class labels. Let $\Gamma({\bf x}_c)$ be the set of unique labels assigned to variables in the clique $c$, and $\omega_c$ be the weight associated with the clique. In our case, we take the clique potentials as proportional to the number of unique labels in the clique $c$:
 
\begin{equation}
  \phi_c({\bf x}_c) = \omega_c|\Gamma({\bf x}_c)|
\label{eq:diversity_potential}
\end{equation}
where the notation $|A|$ denotes the cardinality of a set A.

\myparagraph{LP Relaxation}

First, let us consider the IP formulation of the MAP problem for our diversity model. Using the same set of indicator variables $y_{ai} = \{0, 1\}$ as for the Potts model, which are binary now, the clique potential $\phi({\bf x}_c)$ can be represented as

\begin{equation}
  \phi_c({\bf x}_c) = \omega_c \cdot \sum_{i = 1}^L \max_{(a, b) \in c} |y_{ai} - y_{bi}| 
\label{eq:hod_ip}
\end{equation}

For example, let the set of unique labels in a clique be $\Gamma = \{l_1, l_2, l_3\}$. This implies that $y_{ai} = 1$ for some, but not all, variables in the clique for $i = \{1, 2, 3\}$. Hence, $|y_{ai} - y_{bi}| = 1$ for some pairs $(a, b) \in c$ for $i = \{1, 2, 3\}$. Also, for any other label $i$, $|y_{ai} - y_{bi}| = 0$ for all $(a, b) \in c$ since $y_{ai} = 0$ for all variables $a$ for these labels $i$. For binary variables, the maximum possible value of $|y_{ai} - y_{bi}|$ is 1. Hence, $\phi_c({\bf x}_c) = \omega_c \cdot (1 + 1 + 1) = 3 \cdot \omega_c = \omega_c |\{l_1, l_2, l_3\}$|. 
 
We now relax the indicator variables $y_{ai}$ to lie in $[0, 1]$. Assuming the pairwise potentials in equation (\ref{eq:energy_hop_min}) to be Potts, an accurate LP relaxation for the higher-order diversity model is given by:

\begin{align}
    \text{(HOD-LP)} \quad \min_{\bf y} \enskip & E({\bf y}) =  \sum_{a = 1}^N
    \sum_{i = 1}^L \phi_a(i)y_{ai} + 
    \sum_{(a, b) \in {\mathcal N}} \sum_{i} \frac{w_{ab}}{2} \cdot |y_{ai} - y_{bi}| \nonumber\\
    & + \quad \sum_{c \, \in {\mathcal C}} w_c \cdot \sum_{i = 1}^L \max_{(a, b) \in c} |y_{ai} - y_{bi}| \nonumber\\
    \text{s.t} \quad &{\bf y} \in \Delta.
\label{eq:hop-lp}
\end{align}
where the label (HOD-LP) stands for LP relaxation for higher-order diversity
model. The objective function is similar to that for the Potts model along with
the addition of terms corresponding to higher-order potentials. The constraints
for (HOD-LP) are the same as for (P-LP). The above LP has not been formally
analysed in literature, and we do not make any claims regarding its
optimality. However, its similarity in form to (P-LP) is an indication of its accuracy. We establish its accuracy empirically in section \ref{sec:exp}. 
 
\myparagraph{LP-based Submodular Extension} We use the same $1\text{-of-}L$ encoding
scheme as for Potts model. Problem \ref{eq:min_upper_bound} then factorises as: 
%Same as eq 5.4
\begin{equation}
\min_{{\bf s} \in EP(F)} \quad \sum_{a = 1}^{N} \log \sum_{i = 1}^L \exp(-s_{ai}).
\label{eq:hop_upper_bound}
\end{equation}
We introduce a temperature parameter $T$ to problem (\ref{eq:hop_upper_bound}) to obtain the following new problem:
%Same as eq 5.5
\begin{equation}
    \min_{{\bf s} \in EP(F)} \quad g_T ({\bf s}) = \sum_{a = 1}^{N} T \cdot \log \sum_{i = 1}^L \exp(-\frac{s_{ai}}{T}).
\label{eq:hop_temp}
\end{equation}
In the limit $T \to 0$, problem (\ref{eq:hop_temp}) becomes
%Same as eq 5.6
\begin{align}
    - \min_{{\bf y} \in \Delta} f({\bf y}) 
\end{align}
where $f(.)$ is the Lovasz extension of $F(.)$. We are interested in making
this LP the same as (HOD-LP) of equation (\ref{eq:hop-lp}), thereby enabling us to obtain the submodular extension for the higher-order diversity model. We will make use of the following lemma in our proof. 
{\lemma The Lovasz extension of the following set function 
  \begin{align}
      F_{HOD}(A) &= F_{Potts}(A) + \nonumber \sum_{c \in {\mathcal C}} w_c \cdot \sum_{i = 1}^L \max_{(a, b) \in c} [|A \cap \{v_{ai}, v_{bi}\}| = 1]
\label{hod_submodular}
\end{align}
is the objective function $E({\bf y})$ of the LP relaxation (HOD-LP).
\label{lemma:hod_extension}}
\newline

\begin{proof}
 Making use of proposition \ref{proposition:potts_worst-case optimal}, it will be sufficient for us to show that the Lovasz extension of the $i$-th Ising model $F_i(A) = \max_{(a, b) \in c} [|A \cap \{v_{ai}, v_{bi}\}| = 1]$ is $\max_{(a, b) \in c} |y_{ai} - y_{bi}|$.

Let the clique $c$ have $M$ variables. Given ${\bf y} \in [0, 1]^M$, let us order its components in decreasing order $y_{j_1} > \dots > y_{j_M}$ , where $(j_1,\dots, j_M)$ is a permutation. Then the Lovasz extension of $F_i()$ is:
\begin{equation}
f_i({\bf y}) = \sum_{k = 1}^M y_{j_k} [F_i(\{j_1, \dots, j_k\}) - F_i(\{j_1, \dots, j_{k - 1}\})]
\nonumber
\end{equation}

We note that when $A = \{ \}$ or $\{j_1, \dots, j_M\}$, that is, when $A$ represents homogeneous labelings, $F_i(A) = 0$. For any other $A \subset \{j_1, \dots, j_M\}$, $F_i(A) = 1$. Hence, the Lovasz extension simplifies to
\begin{align}
f_i({\bf y}) &= y_{j_1} - y_{j_M} \nonumber \\
&= \max_{(a, b) \in c} |y_{ai} - y_{bi}| \nonumber
\end{align}
This proves our lemma.
\end{proof}

We are now in a position to state our main result:

{\proposition The set function $F_{HOD}$ of lemma \ref{lemma:hod_extension} is the submodular extension of our higher-order diversity model whose Lovasz extension is $E({\bf y})$ in problem (HOD-LP).} 

\begin{proof}
We first observe that $\forall {\bf y} \in {\mathcal L}^N$, that is, for all valid labelings, $F_{HOD}(A_{\bf y})$ equals $E({\bf y})$.  Hence, $F_{HOD}$ is an extension. 

Also, we showed in lemma \ref{lemma:hod_extension} that the Lovasz extension of
$F_{HOD}$ is $E({\bf y})$ in problem (HOD-LP). $E({\bf y})$ being a sum of terms corresponding to maximum of linear functions, is convex. The convexity of Lovasz extension implies submodularity of the set function \citep{bach2013learning}. Hence, $F_{HOD}$ is submodular. 
\end{proof}
 
We now show the effectiveness of the accurate submodular extensions for different classes of models by means of experiments on synthetic and real-world datasets.

\mysection{Experiments}
\label{sec:exp}
Using synthetic data, we show that our upper-bound compares favorably with TRW
for both Potts and hierarchical Potts models. For comparison, we restrict
ourselves to sparse CRFs, as the code available for TRW does not scale well to
dense CRFs. We also perform stereo matching using dense CRF models and compare
our results with the mean-field-based approach of \citep{koltun2011efficient}.
All experiments were run on a x86-64, 3.8GHz machine with 16GB RAM. In this
section, we refer to our algorithm as \emph{Submod} and mean field as \emph{MF}. 

%\begin{figure}
%       \Tree [.root [.{m0} 0-3 ] [.{m1} 4-7 ] [.{m2} 8-11 ] [.{m3} 12-15 ] [.{m4} 16-19 ]]
%   %    \Tree [.root [.{m0} 0 1 2 3 ] [.{m1} 4 5 6 7 ] [.{m2} 8 9 10 11 ] [.{m3} 12 13 14 15 ] [.{m4} 16 17 18 19 ]]
%%    \Tree[. [. [.1 .2 .3]]]
%\mycaption{\footnotesize \em An $r$-HST defining pairwise distance among 20 labels used for upper-bound comparison with TRW. $m0$ stands for meta-label 0, and so on.}
%\label{fig:syn_rhst}
%\end{figure}
\begin{figure}
\centering
\begin{tabular}{cc}
\includegraphics[scale = 0.30]{./figures/synthetic_tree.png} &
\includegraphics[scale = 0.30]{./figures/synthetic_tree_2_new.png} \\
  \scriptsize(a) Tree A & \scriptsize(b) Tree B \\
\end{tabular}
\mycaption{\footnotesize \em {\bf Trees for synthetic experiments:} The hierarchical Potts models defining pairwise
distance among 20 labels used for upper-bound comparison with TRW. Blue nodes
are the meta-labels and yellow nodes are labels. All the edges at a particular
level have the same edge weights. The sequence of weights from root level to
leaf level is 1, 0.5 for tree A and 1, 1, 0.5 for tree B. The yellow node is
shown to clump together 4 and 5 leaf nodes for tree A and B respectively.} 
\label{fig:syn_rhst}
\end{figure}

\begin{figure}
\centering
\resizebox{190pt}{160pt}{
\begin{tikzpicture}[-latex ,auto ,node distance =2 cm and 2cm ,on grid
  , semithick , state/.style ={ circle , color = black , draw,black , text=blue , minimum width =1 cm}, cross/.style={color=red, very thick, dashed}, unary/.style={color=blue, very thick}] 

  \node[state] (n11) at (1, 1) {\Large $1\mhyphen1$};
  \node[state] (n12) at (5, 1) {\Large $1\mhyphen2$};
  \node[state] (n13) at (3, -1) {\Large $1\mhyphen3$};
  \node[state] (n21) at (7, 1) {\Large $2\mhyphen1$};
  \node[state] (n22) at (11, 1) {\Large $2\mhyphen2$};
  \node[state] (n23) at (9, -1) {\Large $2\mhyphen3$};
  \node[state] (source) at (6, 3) {\Large s};
  \node[state] (sink) at (6, -3) {\Large t};
  
  \path[unary] (n11) edge [bend left =25] node[below] {\Large$\phi_1(1)$} (n12);
\path[unary] (n12) edge [bend left =25] node[left] {\Large$\phi_1(2)$} (n13);
\path[unary] (n13) edge [bend left =25] node[left] {\Large$\phi_1(3)$} (n11);
\path[unary] (n21) edge [bend left =25] node[below] {\Large$\phi_2(1)$} (n22);
\path[unary] (n22) edge [bend left =25] node[right] {\Large$\phi_2(2)$} (n23);
\path[unary] (n23) edge [bend left =25] node[right] {\Large$\phi_2(3)$} (n21);

\path[cross] (n11) edge [bend left =25] node[above] {} (n21);
\path[cross] (n21) edge [bend left =25] node[below] {} (n11);

\path[cross] (n12) edge [bend left =25] node[above] {} (n22);
\path[cross] (n22) edge [bend left =25] node[below] {} (n12);

\path[cross] (n13) edge [bend left =25] node[above] {} (n23);
\path[cross] (n23) edge [bend left =25] node[below] {} (n13);
\end{tikzpicture}
}
\mycaption{\footnotesize \em {\bf Alternate extension for synthetic experiments}: An st-graph specifying the alternate submodular
extension for Potts model for 2 variables with 3 labels each and connected to
each other. The convention used is same as in figure \ref{fig:optimal_extension_graph}. Each dotted red arc has weight $w_{12}/2$.  This alternate extension was also used to derive the extension for hierarchical Potts model.}
\label{fig:alternate_extension}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale = 0.80]{./figures/synthetic_upper_bound_comparison.pdf}
\mycaption{\footnotesize \em {\bf Upper-bound comparison using synthetic data:} The
plot shows the ratio (Submod bound - TRW bound)/|TRW bound| averaged over 100
unary instances as a function of pairwise weights using the worst-case optimal and
alternate extension for Potts and hierarchical Potts models. We observe that
the worst-case optimal extension
(solid) results in tighter bounds as compared to the respective alternate
extensions (dotted). Also, the worst-case optimal extension bounds are in
similar range as the TRW bounds.}
\label{fig:syn_plot}
\end{figure}


\mysubsection{Upper-bound Comparison using Synthetic Data}
\label{subsec:synthetic}
\myparagraph{Data} We generate lattices of size 100 $\times$ 100, where each
lattice point represents a variable taking one of 20 labels. The pairwise
relations of the sparse CRFs are defined by 4-connected neighbourhoods. The
unary potentials are uniformly sampled in the range [0, 10]. We consider (a)
Potts model and (b) hierarchical Potts models with pairwise distance between
labels given by the trees of figure \ref{fig:syn_rhst}. The pairwise weights
are varied in the range $\{1, 2, 5, 10\}$. We compare the results of
our worst-case optimal submodular extension with an alternate submodular
extension as given in figure \ref{fig:alternate_extension}.

\myparagraph{Method} For our algorithm, we use the standard schedule $\gamma
= 2/(k + 2)$ to obtain step size $\gamma$ at iteration $k$. We run our
algorithm till convergence - 100 iterations suffices for this. The experiments are repeated for 100 randomly generated unaries for each model and each weight. For TRW, we used the MATLAB toolbox of \citep{domke2013learning}. The baseline code does not optimise over tree distributions. We varied the edge-appearance probability in trees over the range [0.1 - 0.5] and found 0.5 to give tightest upper bound.

\myparagraph{Results} We plot the ratio of the normalised difference of the upper
bound values of our method with TRW as a function of pairwise weights. The
ratios are averaged over 100 instances of unaries. Figure \ref{fig:syn_plot}
shows the plots for Potts and hierarchical Potts models for the worst-case
optimal and alternate extension. We find that the optimal extension (solid) results 
in tighter upper-bounds than the alternate extension (dotted) for both models.
This is because the representation of the submodular function using figure \ref{fig:alternate_extension} necessitates that $\phi_a(i)$ be non-negative. This implies that $F(A)$ values are larger
for the worst-case optimal extension of figure \ref{fig:optimal_extension_graph} as compared to the
alternate extension. Hence the minimisation problem \ref{eq:min_upper_bound}
has larger domain $EP(F)$ for the optimal extension, thereby resulting in better minima. Figure \ref{fig:syn_plot} also indicates that our algorithm with optimal extension provides similar range of upper bound as TRW, thereby providing empirical justification of our
method. TRW makes use of the standard LP relaxation \citep{chekuri2004linear} which is tighter than Kleinberg-Tardos relaxation, resulting in better approximation. However, TRW does not scale well with neighborhood size, thereby prohibiting its use in dense CRFs.

\begin{figure*}
    \centering
\begin{tabular}{ccc}
        \includegraphics[scale = 0.24]{\imagePath/stereo/venus_GT} &
        \includegraphics[scale = 0.24]{\imagePath/stereo/potts/venus_left_mf} &
        \includegraphics[scale = 0.24]{\imagePath/stereo/potts/venus_left_submod} \\
    %
        \scriptsize(a) Venus GT & \scriptsize(b) MF solution & \scriptsize(c) Submod solution \\ 
        {} & \scriptsize  60.32s, 1.83e+07 &  \scriptsize 469.75s, {\bf 1.55e+07}  \\
        \includegraphics[scale = 0.27]{\imagePath/stereo/tsukuba_GT} &
        \includegraphics[scale = 0.27]{\imagePath/stereo/potts/tsukuba_left_mf} &
        \includegraphics[scale = 0.27]{\imagePath/stereo/potts/tsukuba_left_submod} \\
        \scriptsize(a) Tsukuba GT & \scriptsize(b) MF solution & \scriptsize(c) Submod solution\\
        {} & \scriptsize  14.93s, 8.21e+06 & \scriptsize 215.22s, {\bf 4.12e+06} \\

        \includegraphics[scale = 0.24]{\imagePath/stereo/cones_GT} &
        \includegraphics[scale = 0.24]{\imagePath/stereo/potts/cones_left_mf} &
        \includegraphics[scale = 0.24]{\imagePath/stereo/potts/cones_left_submod} \\
        \scriptsize(a) Cones GT & \scriptsize(b) MF solution & \scriptsize(c) Submod solution \\
        {} & \scriptsize 239.14s, 2.68e+07 &  \scriptsize 1082.72s, {\bf 1.27e+07} \\ 
        %
       %
         \includegraphics[scale = 0.25]{\imagePath/stereo/teddy_GT} &
        \includegraphics[scale = 0.25]{\imagePath/stereo/potts/teddy_left_mf} &
        \includegraphics[scale = 0.25]{\imagePath/stereo/potts/teddy_left_submod} \\
        \scriptsize(a) Teddy GT & \scriptsize(b) MF solution & \scriptsize(c) Submod solution\\
        {} & \scriptsize 555.30s, 2.36e+07 & \scriptsize 1257.86s, {\bf 1.58e+07}\\
\end{tabular}
%\vspace{2mm}
\mycaption{\footnotesize \em Stereo matching using dense CRFs with Potts
compatibility and Gaussian pairwise potentials. We compare our solution with
the mean-field algorithm of Koltun and Krahenbuhl \citep{koltun2011efficient}. We observe that our method gives better-looking solutions with lower energy value at the cost of higher computational time.}
\label{fig:stereo}
\end{figure*}

\clearpage
\mysubsection{Stereo Matching using Dense CRFs}
\label{subsec:stereo}
\myparagraph{Data} We demonstrate the benefit our algorithm for stereo matching on images extracted from the Middlebury stereo matching dataset \citep{scharstein2001taxonomy}. We use dense CRF models with Potts compatibility term and Gaussian pairwise potentials. The unary terms are obtained using the absolute difference matching function of \citep{scharstein2001taxonomy}. 

\myparagraph{Method} We use the implementation of mean-field algorithm for
dense CRFs of \citep{koltun2011efficient} as our baseline. For our algorithm,
we make use of the modified Gaussian filtering implementation for dense CRFs by
\citep{ajanthan2017efficient} to compute the conditional gradient at each step.
The step size $\gamma$ at each iteration is selected by doing line search. We
run our algorithm till 100 iterations, since the visual quality of the solution
does not show much improvement beyond this point. We run mean-field up to
convergence, with a threshold of 0.001 for change in KL-divergence. 

\myparagraph{Results} Figure \ref{fig:stereo} shows some example solutions obtained by picking the
label with maximum marginal probability for each variable for mean-field and
for our algorithm. We also report the time and energy values of the solution
for both methods. Though we are not performing MAP estimation, energy values
give us a quantitative indication of the quality of solutions. For the full set of
21 image pairs (2006 dataset), the average ratio of the energies of the solutions from our method 
compared to mean-field is 0.943. The average time ratio is 10.66. We observe that our algorithm results in more natural looking stereo matching results with lower energy values for all images. However, mean-field runs faster than our method for each instance.The set of hyperparameters that we used can be found in the
appendix.


\begin{figure*}
    \centering
\begin{tabular}{cc}
        \includegraphics[scale = 0.30]{\imagePath/stereo/higher_diversity/tsukuba_GT} &
        \includegraphics[scale = 0.30]{\imagePath/stereo/higher_diversity/tsukuba_hop} \\
        \scriptsize(a) Tsukuba GT & \scriptsize(b) Submod solution \\
        {} & \scriptsize 1582.04s  \\
        \includegraphics[scale = 0.52]{\imagePath/stereo/higher_diversity/baby1_GT} & 
        \includegraphics[scale = 0.26]{\imagePath/stereo/higher_diversity/baby1_hop} \\ 
        \scriptsize(a) Baby1 GT & \scriptsize(b) Submod solution \\
        {} & \scriptsize 2476.82s \\
        \includegraphics[scale = 0.52]{\imagePath/stereo/higher_diversity/flowerpots_GT}&  
        \includegraphics[scale = 0.26]{\imagePath/stereo/higher_diversity/flowerpots_hop} \\
        \scriptsize(a) Flowerpots GT & \scriptsize(b) Submod solution\\
        {} & \scriptsize 2471.58s\\

\end{tabular}
%\vspace{2mm}
\mycaption{\footnotesize \em Stereo matching using dense CRFs with higher-order
  diversity term in addition to Potts compatibility and Gaussian pairwise potentials.}
\label{fig:stereo_hod}
\end{figure*}

\mysubsection{Stereo Matching using Higher-order Diversity Model}
\label{subsec:stereo_hod}
\myparagraph{Data} Next, we use the higher-order diversity model for stereo
matching on the Middlebury dataset. A higher-order model is suitable to be used
for only some images in the dataset, since others have gradually sloping
surfaces and higher-order cliques will force all pixels on these surfaces to
take similar disparity values.  We take a dense CRF model with Potts
compatibility term and Gaussian pairwise potentials, and augment it with
higher-order diversity term. We obtain higher-order cliques by using super-pixels obtained from the mean-shift algorithm \citep{comaniciu2002mean}.

\myparagraph{Method} We again use the modified Gaussian filtering implementation for dense CRFs by
\citep{ajanthan2017efficient} to compute the conditional gradient at each step.
To compute the contribution of the higher-order model, we make use of an
efficient strategy to reuse computation. Recall that computing the conditional
gradient coordinates $s^*_{\sigma(i)}$ requires us to obtain differences of the
nested sets $F(S_i) - F(S_{i - 1})$. We store the clique state at each $i$, and
this enables us to compute the difference for each coordinate in a constant
amount of time. In this case, computation of conditional gradient takes
$\mathcal{O}(cNL)$ where $c$ is the number of higher-order cliques. We ran our
algorithm for 200 iterations.

\myparagraph{Results} Figure \ref{fig:stereo_hod} shows some example solutions
obtained by picking the label with maximum marginal probability for each
variable. We observe that our method gives results that are reasonably close to
the ground-truth. The set of hyperparameters that we used can be found in the
appendix.

\begin{figure*}
    \centering
\begin{tabular}{cc}
  \includegraphics[scale = 1.3]{./figures/semantic/2_14_s} &
  \includegraphics[scale = 0.5]{./figures/semantic/2_14_s_submod} \\
        \scriptsize (a) & \scriptsize (e) \\
  \includegraphics[scale = 1.3]{./figures/semantic/11_4_s} &
  \includegraphics[scale = 0.5]{./figures/semantic/11_4_s_submod} \\
        \scriptsize (b) & \scriptsize (f) \\
  \includegraphics[scale = 1.3]{./figures/semantic/12_7_s} &
  \includegraphics[scale = 0.5]{./figures/semantic/12_7_s_submod} \\
        \scriptsize (c) & \scriptsize (g) \\
  \includegraphics[scale = 1.3]{./figures/semantic/4_16_s} &
  \includegraphics[scale = 0.5]{./figures/semantic/4_16_s_submod} \\
        \scriptsize (d) & \scriptsize (h) \\
\end{tabular}
\mycaption{\footnotesize \em Sample results from semantic segmentation on the
MSRC-21 dataset using dense CRFs with higher-order diversity term in addition
to Potts compatibility and Gaussian pairwise potentials. (a)-(d) show the input
image, and (e)-(f) show the corresponding results using our method. We observe that our
method is able to segment out objects with complex boundaries fairly
accurately.}
\label{fig:semantic}
\end{figure*}


\mysubsection{Semantic Segmentation using Higher-order Diversity Model}
\label{subsec:semantic_hod}
\myparagraph{Data} We evaluate our approach on the task of semantic
segmentation on the MSRC-21 dataset \citep{shotton2009textonboost}. It consists
of 591 color images of size 320$\times$213 with corresponding ground truth
labelings of 21 object classes. We made use of the unary features from the
\textit{TextonBoost} classifier \citep{shotton2009textonboost}. As for stereo
matching (subsection \ref{subsec:stereo_hod}), we augment the dense CRF with
Gaussian pairwise potentials with our higher-order diversity model. Superpixels for higher-order modeling were obtained from the mean-shift algorithm \citep{comaniciu2002mean}.

\myparagraph{Method} As for stereo matching, we used the modified Gaussian filtering implementation \citep{ajanthan2017efficient} for the contribution of the dense pairwise terms to the conditional gradient. We used the same strategy as in subsection \ref{subsec:stereo_hod} to compute the higher-order component of the conditional gradient in $\mathcal{O}(cNL)$ time-complexity. We ran our
algorithm for 100 iterations.

\myparagraph{Results} The ground-truth labelings provided with the MSRC-21
dataset are very coarse. We evaluate our algorithm on the set of 94 images for which fine-grain
annotations are available \citep{koltun2011efficient}. On this subset, our
method correctly labeled 81.18\% of pixels. It took 275.02s on average per
instance to run our method for 100 iterations. Some representative results are
shown in figure \ref{fig:semantic} where we label each pixel with the label
having the maximum marginal probability. Our set of hyperparameters for this dataset can be found in the
appendix.


\mysection{Discussion}
\label{sec:conclude}
We have established the relation between submodular extension and the LP
relaxation for MAP estimation using Lovasz extension for various CRF models. This
allowed us to identify the worst-case optimal submodular extension for Potts as
well as the general metric labeling problems. In addition, we obtained an
accurate submodular extension for a higher-order model based on label-diversity
in cliques. It is worth noting that it might still be possible to obtain an
improved submodular extension for a given problem instance. The design of
a computationally feasible algorithm for this task is an interesting direction
of future research. While our work focused on one class of higher-order model,
there is potential for our approach to be used to identify accurate submodular
extensions for other energy families, such as truncated max-of-convex models \citep{pansari2017truncated}. 
\newpage

